---
title: "Main Analysis"
author: "Astrid Stulemeijer"
date: "2024-04-17"
output: html_document
---

# Preparations
## Loading needed packages, data & scripts
```{r}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

# Load R Packages
library(RSiena)
library(dplyr)
library(ggplot2)
library(igraph)
library(sna)
library(network)
library(RColorBrewer)
library(GGally)
library(reshape2)
library(parallel)
library(writexl)
library(stats)
library(car)
library(ResourceSelection)
library(RColorBrewer)
library(grid)
library(MASS)
library(sna)
library(tidyr)
library(ppcor)

# Load Data
load("../Data/Input/Glasgow-friendship.RData")
load("../Data/Input/Glasgow-demographic.RData")

# Load no-error-result from random scenario to combine with systematic results
scen0_pars <- readRDS("../Results/Systematic/scen0_pars")
scen0_convergence <- readRDS("../Results/Systematic/scen0_convergence")
scen0_netwstats <- readRDS("../Results/Systematic/scen0_netwstats")
scen0_paramstats <- readRDS("../Results/Systematic/scen0_paramstats")

# Source Scripts
source("./SimulationRun.R")
source("./HelperFunctions.R")
source("./FormatOutput.R")
source("./AnalyzeOutput.R")
source("./Graphs.R")

# Set seed
set.seed(3363430)
```

## Data Preparation
```{r}
# Only select actors that are active participants in both wave 2 and 3 
noresponse_1 <- apply(friendship.2, 1, function(row) all(is.na(row) | row == 10))
noresponse_2 <- apply(friendship.3, 1, function(row) all(is.na(row) | row == 10))
inactive <- noresponse_1 | noresponse_2

netwmatrices <- list(friendship.2[!inactive, !inactive], friendship.3[!inactive, !inactive]) %>%
  lapply(., function(x) apply(x,1, function(y) replace(y,y == 2, 1))) # Create network without weights

# Coding sex as 0/1
sex <- ifelse(sex.F[!inactive] == 1, 0, 1)
sex2 <- coCovar(sex, centered = F)

value <- ifelse(friendship.2[!inactive, !inactive] == 2, 1, friendship.2[!inactive, !inactive])
```

## Creating Relevant Objects
```{r}
controls <- sienaAlgorithmCreate(seed = 2809, cond = FALSE)

# Creating "Real" SAOM model & second observations generated based on the second model
if (file.exists("../Data/Output/RealModel.RData")){
  realmodel <- readRDS("../Data/Output/RealModel.RData")
  second_observations <- readRDS("../Data/Output/SecondObservations.RData")
} else{
  
  # Real Model
  friendship <- sienaDependent(array(c(netwmatrices[[1]], netwmatrices[[2]]), 
                                   dim = c(133,133,2)))
  glasgowdata <- sienaDataCreate(friendship, sex2)
  
  effects <- getEffects(glasgowdata)
  effects <- includeEffects(effects, name = "friendship", 
                            density, recip, cycle3, gwespFF, transRecTrip, inPop, inAct)
  effects <- includeEffects(effects, name = "friendship", sameX, interaction1 = "sex2")
  
  realmodel <- siena07(controls, data = glasgowdata, effects = effects)
  
  # Second Observations
  simulation_controls <- sienaAlgorithmCreate(seed = 2809, n3 = 1000, nsub = 0)
  simulations <- siena07(simulation_controls, data = glasgowdata, effects = effects, 
                   returnDeps = T, prevAns = realmodel)
  
  second_observations_list <- lapply(lapply(lapply(simulations$sims, "[[", "Data1"), "[[", "friendship"), 
                                "[[", "1")
  second_observations <- lapply(second_observations_list, EdgelistToMatrix)
  
  
  saveRDS(realmodel, "../Data/Output/RealModel.RData")
  saveRDS(second_observations, "../Data/Output/SecondObservations.RData")
}

```

# Determining error introduction
## Calculation of clustering for observed data
```{r} 
detach("package:sna", unload=TRUE) # SNA package interferes with analysis

graph_undirected_1 <- graph_from_adjacency_matrix(netwmatrices[[1]], mode = "max")
graph_undirected_2 <- graph_from_adjacency_matrix(netwmatrices[[2]], mode = "max")

# Check magnitude of isolated edges problem
vertex_degrees <- degree(graph_undirected_1)

isolated_edges_1 <- E(graph_undirected_1)[
  vertex_degrees[ends(graph_undirected_1, E(graph_undirected_1))[, 1]] == 1 &
    vertex_degrees[ends(graph_undirected_1, E(graph_undirected_1))[, 2]] == 1]

isolated_edges_2 <- E(graph_undirected_2)[
  vertex_degrees[ends(graph_undirected_2, E(graph_undirected_2))[, 1]] == 1 &
    vertex_degrees[ends(graph_undirected_2, E(graph_undirected_2))[, 2]] == 1]

# There do not seem to be isolated edges in the observed data

# Calculate local edge clustering in the two observed networks
clustering_results <- sapply(E(graph_undirected_1), function(x) 
  LocalEdgeClustering(graph_undirected_1, x))
clustering_results2 <- sapply(E(graph_undirected_2), function(x) 
  LocalEdgeClustering(graph_undirected_2, x))
all_clustering <- c(clustering_results, clustering_results2)

hist(clustering_results, breaks = 40)
hist(clustering_results2, breaks = 40)
hist(all_clustering, breaks = 50)
# In network 2, slightly more embeddedness
```

## Checking relation between clustering and reported friendship strength in observed data
```{r}
# Check relation between clustering and reported strength
valuednetwork_strength <- c(t(friendship.2[!inactive, !inactive]), t(friendship.3[!inactive, !inactive]))

clustering_matrix1 <- clustering_matrix2 <- matrix(NA, nrow = 133, ncol = 133)
vertex_names <- V(graph_undirected_1)$name  
rownames(clustering_matrix1) <- rownames(clustering_matrix2) <- vertex_names
colnames(clustering_matrix1) <- colnames(clustering_matrix2) <- vertex_names

for(edge_id in seq_along(E(graph_undirected_1))){
  actor1 <- ends(graph_undirected_1, edge_id)[1]
  actor2 <- ends(graph_undirected_1, edge_id)[2]
  edge_clust_coef <- LocalEdgeClustering(graph_undirected_1, edge_id)
  
  # Only add tie strengths if there is a tie.
  if (netwmatrices[[1]][actor1, actor2] == 1) {
    clustering_matrix1[actor1, actor2] <- edge_clust_coef
  } 
  if (netwmatrices[[1]][actor2, actor1] == 1) {
    clustering_matrix1[actor2, actor1] <- edge_clust_coef
  }
}

for(edge_id in seq_along(E(graph_undirected_2))){
  actor1 <- ends(graph_undirected_2, edge_id)[1]
  actor2 <- ends(graph_undirected_2, edge_id)[2]
  edge_clust_coef <- LocalEdgeClustering(graph_undirected_2, edge_id)
  
  # Only add tie strengths if there is a tie.
  if (netwmatrices[[2]][actor1, actor2] == 1) {
    clustering_matrix2[actor1, actor2] <- edge_clust_coef
  } 
  if (netwmatrices[[2]][actor2, actor1] == 1) {
    clustering_matrix2[actor2, actor1] <- edge_clust_coef
  }
}

clusterednetwork_strength <- c(clustering_matrix1, clustering_matrix2)

# Removes unobserved ties, since reciprocation is assumed in the clustering calculation
remove <- which(is.na(clusterednetwork_strength))
value_strength_clean <- valuednetwork_strength[-remove]
cluster_strength_clean <- clusterednetwork_strength[-remove]

# Check relation between reported strength and clustering
strength_data <- data.frame(
  reported = valuednetwork_strength,
  clustering = clusterednetwork_strength
)

# Determine significance based on permutation-based p values
observed_diff <- mean(strength_data[strength_data$reported == 1, "clustering"]) - 
                 mean(strength_data[strength_data$reported == 2, "clustering"])

# Function to calculate the difference in means for a permuted dataset
permute_diff <- function(data) {
  permuted_data <- data[!is.na(data$clustering),]
  permuted_data$reported <- sample(permuted_data$reported)
  mean(permuted_data[permuted_data$reported == 1, "clustering"]) - 
    mean(permuted_data[permuted_data$reported == 2, "clustering"])
}

# Run permutation and calculate p-value
perm_diffs <- replicate(10000, permute_diff(strength_data))
p_value <- mean(abs(perm_diffs) >= abs(observed_diff))

# Visualization
ggplot(strength_data, aes(x = clustering, fill = as.factor(reported), group = as.factor(reported))) +
  geom_density(alpha = 0.4, colour = NA) +
  theme_minimal() +
  labs(
    x = "Tie-embeddedness",
    y = "Density",
    fill = "Friendship Type") +
  scale_fill_manual(
    values = c("1" = "#7FBC41", "2" = "#8E1652"),
    labels = c("Best Friend", "Just a Friend")) +
  theme(
    axis.title.y = element_text(margin = margin(r = 15)),
    axis.title.x = element_text(margin = margin(t = 15))
  ) +
  coord_fixed(ratio = 0.4)

ggplot(strength_data[!is.na(strength_data$clustering),], aes(x = clustering)) +
  geom_density(alpha = 0.4) +
  theme_classic() +
  labs(
    x = "Tie-embeddedness",
    y = "Density") +
  theme(
    axis.title.y = element_text(margin = margin(r = 15)),
    axis.title.x = element_text(margin = margin(t = 15))
  ) +
  coord_fixed(ratio = 0.4)

strength_data %>%
  group_by(reported) %>%
  summarize(across(
  .cols = c(1), 
  .fns = list(mean = mean, 
               median = median, 
               sd = sd,
               min = min,
               max = max), 
  .names = "{.col}_{.fn}"
  ))
```

## Find best sigmoid curve to approach forgetting values
```{r}
# Quantiles corresponding to negative error probabilities based on brewer & webster
forgetting_probability <- data.frame(
  clustering_value = c(min(all_clustering), quantile(all_clustering, probs = 0.641), 
                       quantile(all_clustering, probs = 0.883)),
  remembering = c(0.74, 0.91, 0.97)
)

# Get retention probabilities for Sigmoid function with steepness = 4.5 and
# varying midpoints
generate_sigmoid_data <- function(k = 4.5, x0){
   x <- seq(0, 1, 0.01)
   
   data <- data.frame(
     x = numeric(),
     k = numeric(),
     x0 = numeric(),
     p = numeric()
   )
   
   sigm_values <- expand.grid(k, x0)
   
   # Calculate sigmoid values for the sigmoid function with given k and x0
   for (i in 1:nrow(sigm_values)){
     
     data_temp <- data.frame(
       x = x,
       k = rep(sigm_values[i, 1], length(x)),
       x0 = rep(sigm_values[i, 2], length(x)),
       p = sapply(x, function(x) 
         SigmoidFunction(x = x, x0 = sigm_values[i, 2], k = sigm_values[i, 1]))
     )
     
     data <- rbind(data, data_temp)
   }
   
   return(data)
}

# Generate sigmoid functions, where x0 ranges from -0.12 and -0.36
# (Relevant values were manually calculated/approximated)
sigmoid_data <- generate_sigmoid_data(x0 = seq(-0.12, -0.36, -0.03))  

# Plot all sigmoid functions
ggplot(sigmoid_data, aes(x = x, y = p, colour = interaction(k, x0), group = interaction(k, x0)))+
  geom_line() +
  labs(x = "Tie-embeddedness", y = "Retention probability",
       colour = expression(v[0])) +
  scale_color_brewer(palette = "Paired",
                     labels = sapply(seq(-0.12, -0.36, -0.03), function(x) sprintf("%.2f", x))) + 
  theme_minimal() +
  theme(
    axis.title.y = element_text(margin = margin(r = 15)),
    axis.title.x = element_text(margin = margin(t = 15))
  ) +
  geom_vline(xintercept = 0, show.legend = FALSE, linetype = "dashed") +
  geom_vline(xintercept = 0.29, show.legend = FALSE, linetype = "dashed") +
  geom_vline(xintercept = 0.5, show.legend = FALSE, linetype = "dashed") +
  xlim(0, 1) +
  coord_fixed(ratio = 2.5)

# Filter out simgoid data that correspond to relevant "borders"
# of friendship categories (based on Brewer & Webster)
check <- sigmoid_data %>%
  filter(x %in% c(0, 0.29, 0.5, 1)) 

write_xlsx(check, "../Results/Systematic/check_sigmoid_results.xlsx")
```

# Systematic Error Simulation
## Settings for simulation
```{r}
# Number of simulations
n <- 1000
# selects both baseline negative error & corresponding x0
neg_error_x0 <- check[check$x == 0, 3:4] 
# convert baselineretention probability to negative error (meaningfull values for reporting)
neg_error_x0$p <- 1 - neg_error_x0$p 
pos_error <-  c(0, 0.001)

# Set up and combine parameters for simulations
simulation_element_systematic <- expand.grid(1:n, neg_error_x0$p, pos_error)
simulation_element_systematic_full <- left_join(simulation_element_systematic,
                                                neg_error_x0,
                                                by = c("Var2" = "p"))

simulation_input_syst <- list(simulation_element_systematic_full, 
                              second_observations, netwmatrices, sex2, controls)
names(simulation_input_syst) <- c("simulation_element", "second_observations", 
                             "netwmatrices", "sex2", "controls")
save("simulation_input_syst", file = "../Data/Input/InputSimulation_Systematic.RData")
```

## Perform simulation [full version in ExecuteSimulationParallel.R for Supercomputer]
```{r}
# Actual simulation executed on supercomputer with ExecuteSimulationParallel.R
# and SimulationRun.R

cl <- makeCluster(5, outfile = "logs_systematic.txt")
clusterExport(cl, c("SafeSimulation", "SimulationRun", "IntroduceError_Systematic_Embeddedness", "second_observations", "netwmatrices", "sex2", "controls", "simulation_element_systematic_full", "ChangeCoding", "LocalEdgeClustering", "SigmoidFunction", "Hamming", "Jaccard"))

clusterEvalQ(cl, {library(igraph); library(RSiena)})

systematic_output <- parApply(cl, MARGIN = 1, X = as.matrix(simulation_element_systematic_full), FUN = function(simulation_element) 
    SimulationRun(IntroduceError_Systematic_Embeddedness, netwmatrices[[1]], 
                  second_observations[[simulation_element[1]]], sex2,
                  controls, simulation_element))

stopCluster(cl)
```

## Output Formatting
```{r}
# Retrieve output from supercomputer, and load/calculate corresponding formatted data and results
systematic_output <- readRDS("../Data/Output/02_Systematic_Raw_Output.RData")
error_combo <- expand.grid(neg_error_x0$p, pos_error) 

if (file.exists("../Data/Output/02_Systematic_Formatted_Output.RData")){
  systematic_formatted <- readRDS("../Data/Output/02_Systematic_Formatted_Output.RData")
  systematic_results <- readRDS("../Data/Output/02_Systematic_Results_Work.RData") # less memory
  systematic_results_all <- readRDS("../Data/Output/02_Systematic_Results.RData")
  
} else{
  # Format output so it can be analyzed more easily
  systematic_formatted <- FormatOutput(systematic_output)
  saveRDS(systematic_formatted, "../Data/Output/02_Systematic_Formatted_Output.RData")
  
  # Most important analyses, defined beforehand
  library(sna)
  systematic_results <- AnalyzeOutput(systematic_output, systematic_formatted, realmodel, n, error_combo)
  saveRDS(systematic_results, "../Data/Output/02_Systematic_Results.RData")
  systematic_results_work <- systematic_results[!names(systematic_results) %in% "filtered_output"]
  saveRDS(systematic_results_work, "../Data/Output/02_Systematic_Results_Work.RData")
  
  # SNA package interferes with some parts of analysis
  # but is needed for AnalyzeOuptut()
  detach("package:sna", unload=TRUE) 
}
```

# Analysis
## Save basic results
```{r}
write_xlsx(list(
  NetworkStats = systematic_results_work$summary_networkstats,
  Parameters = systematic_results_work$summary_theta,
  Convergence = systematic_results_work$convergencestats,
  Change = systematic_results_work$change_statistics),
  "../Results/Systematic/Simulation_SystematicErrors.xlsx")
``` 

## Basic density graphs
```{r}
# Graph Dimensions per Parameter to automatically make comparable graphs
dimensions <- list(
  # Order: x axis, y axis
  c(6, 12),   # Rate 
  c(-3.5, 0), # Density
  c(1.5, 3.5), # Reciprocity
  c(-2, 0), # Trans Recip
  c(0, 2), # 3-cycle
  c(1.5, 4), # GWESP
  c(-0.5, 0), # Ind pop
  c(-0.8, -0.1), # Ind act
  c(0.2, 1.5), # Homophily
  c(0, 0.85),
  c(0, 1.75),
  c(0, 2.5),
  c(0, 4),
  c(0, 4),
  c(0, 2.5),
  c(0, 11),
  c(0, 8),
  c(0, 4)
)

# Set up data in the same structure as in systematic_results, including the 0,0 scenario
densitygraph_data <- list(
  summary_theta = rbind(systematic_results_all$summary_theta, scen0_paramstats),
  filtered_output = list(
    theta = rbind(systematic_results_all$filtered_output$theta, scen0_pars)
  )
)

# Selection of negative error values
error_neg_to_display <- unique(rbind(error_combo, c(0, 0))[,1])[c(1,3,5,7,9,10)]

# Density graphs per parameter - with and without positive error
sapply(1:9, function(x) 
  SaveGraph(variable = systematic_results_all$summary_theta$Parameter[x], error_pos = 0, 
            directory = "../Results/Systematic/", 
            random_results = densitygraph_data, error_neg = error_neg_to_display,
            dimensions_x = dimensions[[x]], dimensions_y = dimensions[[x+9]]))

sapply(1:9, function(x) 
  SaveGraph(variable = systematic_results_all$summary_theta$Parameter[x], error_pos = 0.001, 
            directory = "../Results/Systematic/", 
            random_results = densitygraph_data, error_neg = error_neg_to_display,
            dimensions_x = dimensions[[x]], dimensions_y = dimensions[[x+9]]))
```

## Check baseline neg error relation to total error
```{r}
# assess if negative baseline error is a good indicator of the sigmoid curve
pcor.test(tot_error_vals$Error_Total, tot_error_vals$Error_Neg, tot_error_vals$Error_Pos)
plot(tot_error_vals$Error_Total ~ tot_error_vals$Error_Neg)
```

## Convergence analysis
### Format data
```{r}
# Assess number of  succesfull initial runs and reruns, and subsequently the
# initial convergence and initial rerun probabilities
reruns_byerror_all <- as.data.frame(cbind(systematic_formatted$error_neg,
                            systematic_formatted$error_pos, 
                            systematic_formatted$convergence_reruns))
colnames(reruns_byerror_all) <- c("Error_Neg", "Error_Pos", "Rerun")

rerun_stats <- split(reruns_byerror_all, ~ as.factor(Error_Neg) + as.factor(Error_Pos)) %>%
  sapply(., function(x) c(x$Error_Neg[1], x$Error_Pos[1], sum(x$Rerun))) %>%
  t() %>%
  as.data.frame()

colnames(rerun_stats) <- c("Error_Neg", "Error_Pos", "Total_Reruns")

reruns_succesful <- split(reruns_byerror_all[-systematic_results_all$noconvergence_indices,], 
                          ~ as.factor(Error_Neg) + as.factor(Error_Pos)) %>%
  sapply(., function(x) sum(x$Rerun)) 

rerun_stats$Rerun_Success <- reruns_succesful
rerun_stats <- rerun_stats %>%
  mutate(Perc_Success = Rerun_Success/Total_Reruns,
         Initial_Success = (1000 - Total_Reruns)/1000,
         Ok_Reruns_Needed = Rerun_Success/(1000 - (Total_Reruns - Rerun_Success)),
         Total_Useable = 1000-Total_Reruns+Rerun_Success)

rownames(rerun_stats) <- NULL

# Combine with 0 scenario from the random scenario
rerun_stats <- rbind(rerun_stats, scen0_convergence)
write_xlsx(rerun_stats, "../Results/Systematic/RerunStats.xlsx")
```

### Assess data - Initial success
```{r}
# Assess initial convergence probabilities per error combination using
# linear regrsesion and visualizations

# Center negative error because of interactions
rerun_stats$Error_Neg_C <- scale(rerun_stats$Error_Neg, center = T, scale = F)
rerun_stats$Error_Pos <- as.factor(rerun_stats$Error_Pos)

# Check effect of error on success of initial runs 
conv_model <- glm(cbind((1000 - Total_Reruns), Total_Reruns) ~ Error_Neg_C * Error_Pos,
                         data = rerun_stats, family = binomial)

# Check assumptions
vif(conv_model) 

# There are some outliers
rerun_stats$fittedprobInitial <- predict(conv_model, type = "response") 
ggplot(rerun_stats, aes(x = fittedprobInitial, y = Initial_Success)) + 
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(title = "Observed vs. Predicted Proportions",
       x = "Predicted Proportion",
       y = "Observed Proportion") +
  theme_minimal()

residuals <- residuals(conv_model, type = "deviance")
plot(predict(conv_model), residuals)

cooksD <- cooks.distance(conv_model)
plot(cooksD, type = "h")

# Disperson is ok - 1
deviance(conv_model)/df.residual(conv_model)

# Re-estimate model w/o outliers to see what happens
rerun_stats_filtered <- rerun_stats[cooksD <= 4/20, ]
conv_model_filter <- glm(cbind((1000 - Total_Reruns), Total_Reruns) ~ Error_Neg_C * Error_Pos,
                         data = rerun_stats_filtered, family = binomial)

# Effects become smaller and insignificant when removing outliers
rerun_stats_filtered$fittedprobInitial <- predict(conv_model_filter, type = "response") 

ggplot(rerun_stats_filtered, aes(x = Error_Neg, y = Initial_Success, group = factor(Error_Pos), 
                             colour = Error_Pos)) +
  geom_point(size = 2) +
  geom_line(aes(y = fittedprobInitial), linetype = "dashed", size = 1) +
  scale_color_manual(values = c("#8E1652", "#7FBC41"), labels = c("0.000", "1.000")) +
  ylim(0.55, 0.675) + 
  labs(x = " (Baseline) Negative Error",
       y = "Proportion Converged Initial Runs",
       colour = "Positive Error") +
  coord_fixed(ratio = 3) +
  theme_minimal() +
  theme(axis.title.y = element_text(margin = margin(r = 20)),
        axis.title.x = element_text(margin = margin(t = 15)))

# Test model overall significance
conv_null <- glm(cbind((1000 - Total_Reruns), Total_Reruns) ~ 1,
                         data = rerun_stats, family = binomial)
anova(conv_null, conv_model, test = "Chisq")

# Display effects of the full model (including outliers)
ggplot(rerun_stats, aes(x = Error_Neg, y = Initial_Success, group = factor(Error_Pos), 
                             colour = Error_Pos)) +
  geom_point(size = 2) +
  geom_line(aes(y = fittedprobInitial), linetype = "dashed", size = 1) +
  scale_color_manual(values = c("#8E1652", "#7FBC41"), labels = c("0.000", "0.001")) +
  ylim(0.55, 0.675) + 
  labs(x = "Negative Error (Baseline)",
       y = "Proportion Converged Initial Runs",
       colour = "Positive Error") +
  coord_fixed(ratio = 3.2) +
  theme_minimal() +
  theme(axis.title.y = element_text(margin = margin(r = 20)),
        axis.title.x = element_text(margin = margin(t = 15)))
```

#### Assess data - Reruns
```{r}
# Check effect of error on success of reruns
rerun_model2 <- glm(cbind(Rerun_Success, c((Total_Reruns - Rerun_Success))) ~ Error_Neg_C * Error_Pos,
                         data = rerun_stats, family = binomial)

rerun_stats$fittedprobRerun <- predict(rerun_model2, type = "response")

# Check model fit and assumptions
ggplot(rerun_stats, aes(x = fittedprobRerun, y = Perc_Success)) + 
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(title = "Observed vs. Predicted Proportions",
       x = "Predicted Proportion",
       y = "Observed Proportion") +
  theme_minimal()

vif(rerun_model2)

residuals <- residuals(rerun_model2, type = "deviance")
plot(predict(rerun_model2), residuals)

cooksD <- cooks.distance(conv_model)
plot(cooksD, type = "h")

# Dispersion is ok: 0.8
deviance(rerun_model2)/df.residual(rerun_model2)

# Rerun without outliers to see what happens
# No meaningfull differences
rerun_stats_filtered2 <- rerun_stats[cooksD <= 4/20, ]
rerun_model_filter <- glm(cbind(Rerun_Success, c((Total_Reruns - Rerun_Success))) ~ Error_Neg_C * Error_Pos,
                         data = rerun_stats_filtered2, family = binomial)
rerun_stats_filtered2$fittedprobInitial <- predict(rerun_model_filter, type = "response") 

ggplot(rerun_stats_filtered2, aes(x = Error_Neg, y = Perc_Success, group = factor(Error_Pos), 
                             colour = Error_Pos)) +
  geom_point(size = 2) +
  geom_line(aes(y = fittedprobInitial), linetype = "dashed", size = 1) +
  scale_color_manual(values = c("#8E1652", "#7FBC41"), labels = c("0.000", "1.000")) +
  ylim(0.8, 0.95) +
  labs(x = "Negative Error",
       y = "Proportion Converged Initial Runs",
       colour = "Positive Error") +
  coord_fixed(ratio = 2) +
  theme_minimal() +
  theme(axis.title.y = element_text(margin = margin(r = 20)),
        axis.title.x = element_text(margin = margin(t = 15)))

# Model significance
rerun_null <- glm(cbind(Rerun_Success, c((Total_Reruns - Rerun_Success))) ~ 1,
                         data = rerun_stats, family = binomial)
anova(rerun_null, rerun_model2, test = "Chisq")

# Display effects of full model (incl outliers)
ggplot(rerun_stats, aes(x = Error_Neg, y = Perc_Success, group = factor(Error_Pos), 
                             colour = Error_Pos)) +
  geom_point(size = 2) +
  geom_line(aes(y = fittedprobRerun), linetype = "dashed", size = 1) +
  scale_color_manual(values = c("#8E1652", "#7FBC41"), labels = c("0.000", "0.001")) +
  ylim(0.825, 0.9) + 
  labs(x = "(Baseline) Negative Error",
       y = "Proportion Converged Reruns",
       colour = "Positive Error") +
  coord_fixed(ratio = 5) +
  theme_minimal() +
  theme(axis.title.y = element_text(margin = margin(r = 20)),
        axis.title.x = element_text(margin = margin(t = 15)))
```

### Assess rerun data - error vs no error
```{r}
# Test significance of negative error effect, no matter the amount of error
# Without positive error
rerun_stats_error <- rerun_stats[, 1:4] %>%
  mutate(NoError = ifelse(Error_Neg == 0 & Error_Pos == "0", 1, 0)) %>%
  group_by(NoError) %>%
  summarize(
    Total_Reruns = sum(Total_Reruns),
    Rerun_Success = sum(Rerun_Success),
    Initial_Success = (1000*n()-Total_Reruns)/(1000*n()),
    Second_Success = Rerun_Success/Total_Reruns,
    n = n()*1000
  )

prop.test((rerun_stats_error$n-rerun_stats_error$Total_Reruns), rerun_stats_error$n)
prop.test((rerun_stats_error$Rerun_Success), rerun_stats_error$Total_Reruns)

# With positive error
rerun_stats_error <- rerun_stats[, 1:4] %>%
  mutate(NoError = ifelse(Error_Neg == 0 & Error_Pos == "0", 1, 0)) %>%
  group_by(NoError) %>%
  summarize(
    Total_Reruns = sum(Total_Reruns),
    Rerun_Success = sum(Rerun_Success),
    Initial_Success = (1000*n()-Total_Reruns)/(1000*n()),
    Second_Success = Rerun_Success/Total_Reruns,
    n = n()*1000
  )

prop.test((rerun_stats_error$n-rerun_stats_error$Total_Reruns), rerun_stats_error$n)
prop.test((rerun_stats_error$Rerun_Success), rerun_stats_error$Total_Reruns)

# Both with and without positive error: significant effect on initial convergence,
# but not on rerun convergence
# Indicates that convergence simply needs more time/iterations
```  

## SAOM parameter analysis
### Regression of SAOM parameters
```{r}
all_par_data <- rbind(systematic_results_all$filtered_output$theta, scen0_pars)
all_par_data$Error_Pos <- as.factor(all_par_data$Error_Pos)
all_par_data$Error_Neg_C <- scale(all_par_data$Error_Neg, center = T, scale = F)

###### Rate ######
# Assumptions seem ok - some small deviations from normality in tails but ok with large n
rate_model <- lm((Rate-realmodel$theta[1])/realmodel$theta[1] ~ Error_Neg_C * Error_Pos, data = all_par_data)
plot(rate_model)

vif(rate_model) # seems to be sufficient

###### Density ######
# Assumptions seem ok 
density_model <- lm((Density-realmodel$theta[2])/realmodel$theta[2] ~ Error_Neg_C * Error_Pos, data = all_par_data)
plot(density_model)

###### Reciprocity ######
reciprocity_model <- lm((Reciprocity-realmodel$theta[3])/realmodel$theta[3] ~ Error_Neg_C * Error_Pos, data = all_par_data)
plot(reciprocity_model)

###### Trans rec triplets ######
rectrip_model <- lm((`Transitivity-Reciprocity`-realmodel$theta[4])/realmodel$theta[4] ~ Error_Neg_C * Error_Pos, data = all_par_data)
plot(rectrip_model)

###### 3-cycle ######
cycle3_model <- lm((`3-cycles`-realmodel$theta[5])/realmodel$theta[5] ~ Error_Neg_C * Error_Pos, data = all_par_data)
plot(cycle3_model)

###### GWESP ######
# Small deviation in normality in the tail, but not large enough to be of large concern
gwesp_model <- lm((GWESP-realmodel$theta[6])/realmodel$theta[6] ~ Error_Neg_C * Error_Pos, data = all_par_data)
plot(gwesp_model)

###### Indegree - Popularity ######
# Residuals > 2 deviate from normality line 
indpop_model <- lm((`Indegree Popularity`-realmodel$theta[7])/realmodel$theta[7] ~ Error_Neg_C * Error_Pos, data = all_par_data)
plot(indpop_model)

# Check for differences with log-transformed dependent variable
# The fit of the original model is better. It would also not lead to largely different conclusions
# The original model is therefore the final model, for the sake of interpretability
all_par_data$indpop <- (all_par_data$`Indegree Popularity`-realmodel$theta[7])/realmodel$theta[7]
all_par_data$indpop_2 <- all_par_data$indpop + abs(min(all_par_data$indpop)) + 1
indpop_model2 <- lm(indpop_2 ~ Error_Neg_C * Error_Pos, data = all_par_data)
boxcox_result <- boxcox(indpop_model2)
boxcox_result$x[which.max(boxcox_result$y)] # 0.101
indpop_model3 <- lm(log(indpop_2) ~ Error_Neg_C * Error_Pos, data = all_par_data)
plot(indpop_model3)

###### Indegree Activity ######
# Same scenario as above - use original model
indact_model <- lm((`Indegree Activity`-realmodel$theta[8])/realmodel$theta[8] ~ Error_Neg_C * Error_Pos, data = all_par_data)
plot(indact_model)

all_par_data$indact <- (all_par_data$`Indegree Activity`-realmodel$theta[8])/realmodel$theta[8]
all_par_data$indact_2 <- all_par_data$indact + abs(min(all_par_data$indact)) + 1
indact_model2 <- lm(indact_2 ~ Error_Neg_C * Error_Pos, data = all_par_data)
boxcox_result <- boxcox(indact_model2)
boxcox_result$x[which.max(boxcox_result$y)] # -0.02
indact_model3 <- lm(log(indact_2) ~ Error_Neg_C * Error_Pos, data = all_par_data)
plot(indact_model3)

###### Sex homophily ######
# Same scenario as two above. 
hom_model <- lm((`Homophily-Sex`-realmodel$theta[9])/realmodel$theta[9] ~ Error_Neg_C * Error_Pos, data = all_par_data)
plot(hom_model)

all_par_data$hom <- (all_par_data$`Homophily-Sex`-realmodel$theta[9])/realmodel$theta[9]
all_par_data$hom_2 <- all_par_data$hom + abs(min(all_par_data$hom)) + 1
hom_model2 <- lm(hom_2 ~ Error_Neg_C * Error_Pos, data = all_par_data)
boxcox_result <- boxcox(hom_model2)
boxcox_result$x[which.max(boxcox_result$y)] # -0.101
hom_model3 <- lm(log(hom_2) ~ Error_Neg_C * Error_Pos, data = all_par_data)
plot(hom_model3)

prediction_object <- unique(all_par_data[, c(10,11,13)])
# Apply predictions for each model to the prediction_object
prediction_object$rate <- predict(rate_model, prediction_object)
prediction_object$density <- predict(density_model, prediction_object)
prediction_object$reciprocity <- predict(reciprocity_model, prediction_object)
prediction_object$rectrip <- predict(rectrip_model, prediction_object)
prediction_object$cycle3 <- predict(cycle3_model, prediction_object)
prediction_object$gwesp <- predict(gwesp_model, prediction_object)
prediction_object$indpop <- predict(indpop_model, prediction_object)
prediction_object$indact <- predict(indact_model, prediction_object)
prediction_object$hom <- predict(hom_model, prediction_object)
```

### Collect all p values for holm correction + calculate correction
```{r}
# Collect p-values for both random and systematic scenario
P_vals_syst <- c(
  summary(rate_model)$coefficients[,4],
  summary(density_model)$coefficients[,4],
  summary(reciprocity_model)$coefficients[,4],
  summary(rectrip_model)$coefficients[,4],
  summary(cycle3_model)$coefficients[,4],
  summary(gwesp_model)$coefficients[,4],
  summary(indpop_model)$coefficients[,4],
  summary(indact_model)$coefficients[,4],
  summary(hom_model)$coefficients[,4]
)

name_dependent <- rep(names(all_par_data)[1:9], each = 4)
var_names <- rep(c("Intercept", "Neg", "Pos", "X"), 9)

names(P_vals_syst) <- sapply(1:length(name_dependent), 
                               function(i) paste0("Sys_", name_dependent[i], "_", var_names[i]))

saveRDS(P_vals_syst, "../Results/Systematic/PvalsSyst.RData")

P_vals_random <- readRDS("../Results/Random/PvalsRandom.RData")
P_vals_all <- c(P_vals_random, P_vals_syst)

# Adjust p-values 
P_vals_adjusted <- p.adjust(P_vals_all, method = "holm")

P_vals_summary <- data.frame(
  original = P_vals_all,
  holm_corrected = P_vals_adjusted,
  certain_identifier = names(P_vals_all),
  dep_variable = rep(name_dependent, 2),
  predictor = rep(var_names, 2)
)

write_xlsx(P_vals_summary, "../Results/HolmCorrected_P.xlsx")
```

## Sign changes check
```{r}
for (i in 1:9){
  colname <- paste0(colnames(all_par_data)[i], "_signchange")
  real_value <- systematic_results$summary_theta$Real_Theta[i]
  
  # check if sign the same to true parameter
  all_par_data[[colname]] <- ifelse((all_par_data[,i] * real_value) > 0, 0, 1)
}

sign_results <- all_par_data[, c(10, 11, 20:28)] %>%
  group_by(Error_Neg, Error_Pos) %>%
  summarize(across(ends_with("_signchange"), mean, .names = "mean_{col}"))
``` 

## SAOM summary graphs
```{r}
# No positive error
# Coverage
data <- rbind(systematic_results$summary_theta, scen0_paramstats)
data$Parameter <- factor(data$Parameter, levels = unique(data$Parameter))
data$Variance_std <- ave((data$SE^2), data$Parameter, FUN = function(x) (x - mean(x)) / sd(x))
ggplot(data[data$Error_Pos == 0,], 
       aes(x = Error_Neg, y = Coverage, group = as.factor(Parameter), 
           colour = as.factor(Parameter))) +
  geom_point(size = 2) +
  geom_line(linewidth = 1) +
  theme_minimal() +
  geom_hline(yintercept = 0.95) + 
  scale_color_brewer(palette = "Paired") + 
  labs(x = "Negative Error (Baseline)", y = "Coverage", colour = "Parameter") +
  coord_fixed(ratio = 0.35) +
  lims(y = c(0, 1)) +
  theme(axis.title.y = element_text(margin = margin(r = 15), size = 12),
      axis.title.x = element_text(margin = margin(t = 15), size = 12),
      legend.position = "none",
      axis.text.x = element_text(size = 11),
      axis.text.y = element_text(size = 11))

# Relative bias
ggplot(data[data$Error_Pos == 0,], 
       aes(x = Error_Neg, y = Rel_Bias, group = as.factor(Parameter), 
           colour = as.factor(Parameter))) +
  geom_point(size = 2) +
  geom_line(linewidth = 1) +
  scale_color_brewer(palette = "Paired") + 
  theme_minimal() +
  labs(x = "Negative Error (Baseline)", y = "Relative Bias (%)", colour = "Parameter") +
  lims(y = c(-20, 75)) +
  coord_fixed(ratio = 0.0033) +
  theme(axis.title.y = element_text(margin = margin(r = 15), size = 12),
      axis.title.x = element_text(margin = margin(t = 15), size = 12),
      legend.position = "none",
      axis.text.x = element_text(size = 11),
      axis.text.y = element_text(size = 11))

# Variance
ggplot(data[data$Error_Pos == 0,], 
       aes(x = Error_Neg, y = Variance_std, group = as.factor(Parameter), 
           colour = as.factor(Parameter))) +
  geom_point(size = 2) +
  geom_line(linewidth = 1) +
  scale_color_brewer(palette = "Paired") + 
  theme_minimal() +
  labs(x = "Negative Error (Baseline)", y = "Standardized Variance", colour = "Parameter") +
  lims(y = c(-2.5, 2.5)) +
  coord_fixed(ratio = 0.065) +
  theme(axis.title.y = element_text(margin = margin(r = 15), size = 12),
      axis.title.x = element_text(margin = margin(t = 15), size = 12),
      legend.position = "none",
      axis.text.x = element_text(size = 11),
      axis.text.y = element_text(size = 11))

# Percentile Rank
ggplot(data[data$Error_Pos == 0,], 
       aes(x = Error_Neg, y = Percentile_Rank, group = as.factor(Parameter), 
           colour = as.factor(Parameter))) +
  geom_point(size = 2) +
  geom_line(linewidth = 1) +
  theme_minimal() +
  geom_hline(yintercept = 0.5) + 
  scale_color_brewer(palette = "Paired") + 
  labs(x = "Negative Error (Baseline)", y = "Percentile Rank", colour = "Parameter") +
  coord_fixed(ratio = 0.35) +
  lims(y = c(0, 1)) +
  theme(axis.title.y = element_text(margin = margin(r = 15), size = 12),
      axis.title.x = element_text(margin = margin(t = 15), size = 12),
      legend.position = "none",
      axis.text.x = element_text(size = 11),
      axis.text.y = element_text(size = 11))

# With positive error
# Coverage
ggplot(data[data$Error_Pos == 0.001,], 
       aes(x = Error_Neg, y = Coverage, group = as.factor(Parameter), 
           colour = as.factor(Parameter))) +
  geom_point(size = 2) +
  geom_line(linewidth = 1) +
  theme_minimal() +
  geom_hline(yintercept = 0.95) + 
  scale_color_brewer(palette = "Paired") + 
  labs(x = "Negative Error (Baseline)", y = "Coverage", colour = "Parameter") +
  coord_fixed(ratio = 0.35) +
  lims(y = c(0, 1)) +
  theme(axis.title.y = element_text(margin = margin(r = 15), size = 12),
      axis.title.x = element_text(margin = margin(t = 15), size = 12),
      legend.position = "none",
      axis.text.x = element_text(size = 11),
      axis.text.y = element_text(size = 11))

# Relative bias
data$Parameter <- factor(data$Parameter, levels = unique(data$Parameter))
ggplot(data[data$Error_Pos == 0.001,], 
       aes(x = Error_Neg, y = Rel_Bias, group = as.factor(Parameter), 
           colour = as.factor(Parameter))) +
  geom_point(size = 2) +
  geom_line(linewidth = 1) +
  scale_color_brewer(palette = "Paired") + 
  theme_minimal() +
  labs(x = "Negative Error (Baseline)", y = "Relative Bias (%)", colour = "Parameter") +
  lims(y = c(-20, 75)) +
  coord_fixed(ratio = 0.0033) +
  theme(axis.title.y = element_text(margin = margin(r = 15), size = 12),
      axis.title.x = element_text(margin = margin(t = 15), size = 12),
      legend.position = "none",
      axis.text.x = element_text(size = 11),
      axis.text.y = element_text(size = 11))

# Variance
data$Parameter <- factor(data$Parameter, levels = unique(data$Parameter))
ggplot(data[data$Error_Pos == 0.001,], 
       aes(x = Error_Neg, y = Variance_std, group = as.factor(Parameter), 
           colour = as.factor(Parameter))) +
  geom_point(size = 2) +
  geom_line(linewidth = 1) +
  scale_color_brewer(palette = "Paired") + 
  theme_minimal() +
  labs(x = "Negative Error Rate (Baseline)", y = "Standardized Variance", colour = "Parameter") +
  lims(y = c(-2.5, 2.5)) +
  coord_fixed(ratio = 0.065) +
  theme(axis.title.y = element_text(margin = margin(r = 15), size = 12),
      axis.title.x = element_text(margin = margin(t = 15), size = 12),
      legend.position = "none",
      axis.text.x = element_text(size = 11),
      axis.text.y = element_text(size = 11))

# Percentile Rank
ggplot(data[data$Error_Pos == 0.001,], 
       aes(x = Error_Neg, y = Percentile_Rank, group = as.factor(Parameter), 
           colour = as.factor(Parameter))) +
  geom_point(size = 2) +
  geom_line(linewidth = 1) +
  theme_minimal() +
  geom_hline(yintercept = 0.5) + 
  scale_color_brewer(palette = "Paired") + 
  labs(x = "Negative Error (Baseline)", y = "Percentile Rank", colour = "Parameter") +
  coord_fixed(ratio = 0.35) +
  lims(y = c(0, 1)) +
  theme(axis.title.y = element_text(margin = margin(r = 15), size = 12),
      axis.title.x = element_text(margin = margin(t = 15), size = 12),
      legend.position = "none",
      axis.text.x = element_text(size = 11),
      axis.text.y = element_text(size = 11))
```

## Network graphs
### Statistics by error combination
```{r}
# Gather and format all relevant data, including the 0 scenario
networkstat_data <- rbind(systematic_results_all$summary_networkstats, scen0_netwstats)
to_visualize <- networkstat_data[,-c(8:12)] %>%
  mutate(network = rep(c("obs1", "obs2", "an1", "an2"), 20),
         
         # Standardize statistics to 0-1 scale to fit in the graph
         Components = (Components - min(Components, na.rm = TRUE)) / 
           (max(Components, na.rm = TRUE) - min(Components, na.rm = TRUE)),
         Med_Distance = ifelse(is.infinite(Med_Distance), NA, Med_Distance), 
         Med_Distance = (Med_Distance - min(Med_Distance, na.rm = TRUE)) / 
           (max(Med_Distance, na.rm = TRUE) - min(Med_Distance, na.rm = TRUE))) %>%
  filter(!(network %in% c("obs1", "obs2"))) %>%
  pivot_longer(., cols = 3:7, names_to = "Statistic", values_to = "Value")

# Start network - no positive error
ggplot(to_visualize[to_visualize$network == "an1" & to_visualize$Error_Pos == 0,],
       aes(x = Error_Neg, y = Value, group = Statistic, colour = Statistic)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  theme_minimal() +
  scale_color_discrete(labels = c("Components \n (0-1 scaled)", "Density",
                                  "Med Distance \n (0-1 scaled)", "Reciprocity",
                                  "Transitivity")) +
  labs(x = "(Baseline) Negative Error Rate") +
  theme(
    axis.title.y = element_text(margin = margin(r = 15)),
    axis.title.x = element_text(margin = margin(t = 15)),
    legend.title.align = 0,          
    legend.text.align = 0,           
    legend.spacing.y = unit(5, "cm"),
    legend.text = element_text(size = 9),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10)
  ) +
  coord_fixed(ratio = 0.35) +
  lims(y = c(0,1))

# End network - no positive error
ggplot(to_visualize[to_visualize$network == "an2" & to_visualize$Error_Pos == 0,],
       aes(x = Error_Neg, y = Value, group = Statistic, colour = Statistic)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  theme_minimal() +
  scale_color_discrete(labels = c("Components \n (0-1 scaled)", "Density",
                                  "Med Distance \n (0-1 scaled)", "Reciprocity",
                                  "Transitivity")) +
  labs(x = "(Baseline) Negative Error Rate") +
  theme(
    axis.title.y = element_text(margin = margin(r = 15)),
    axis.title.x = element_text(margin = margin(t = 15)),
    legend.title.align = 0,          
    legend.text.align = 0,           
    legend.spacing.y = unit(5, "cm"),
    legend.text = element_text(size = 9),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10)
  ) +
  coord_fixed(ratio = 0.35) +
  lims(y = c(0,1))

# Start network - positive error
ggplot(to_visualize[to_visualize$network == "an1" & to_visualize$Error_Pos == 0.001,],
       aes(x = Error_Neg, y = Value, group = Statistic, colour = Statistic)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  theme_minimal() +
  scale_color_discrete(labels = c("Components \n (0-1 scaled)", "Density",
                                  "Med Distance \n (0-1 scaled)", "Reciprocity",
                                  "Transitivity")) +
  labs(x = "(Baseline) Negative Error Rate") +
  coord_fixed(ratio = 0.35) +
  theme(
    axis.title.y = element_text(margin = margin(r = 15)),
    axis.title.x = element_text(margin = margin(t = 15)),
    legend.title.align = 0,          
    legend.text.align = 0,           
    legend.spacing.y = unit(5, "cm"),
    legend.text = element_text(size = 9),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10)
  ) +
  lims(y = c(0,1))

# End network - positive error
ggplot(to_visualize[to_visualize$network == "an2" & to_visualize$Error_Pos == 0.001,],
       aes(x = Error_Neg, y = Value, group = Statistic, colour = Statistic)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  theme_minimal() +
  scale_color_discrete(labels = c("Components \n (0-1 scaled)", "Density",
                                  "Med Distance \n (0-1 scaled)", "Reciprocity",
                                  "Transitivity")) +
  labs(x = "(Baseline) Negative Error Rate") +
  theme(
    axis.title.y = element_text(margin = margin(r = 15)),
    axis.title.x = element_text(margin = margin(t = 15)),
    legend.title.align = 0,          
    legend.text.align = 0,           
    legend.spacing.y = unit(5, "cm"),
    legend.text = element_text(size = 9),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10)
  ) +
  coord_fixed(ratio = 0.35) +
  lims(y = c(0,1))

```

### Network visualizations
```{r}
# Select networks with neg error rate = 0.36, with network index 2
index_visualizations <- c(2, 9002)

# both had a rerun, but converged on the second attempt
systematic_formatted$convergence_reruns[index_visualizations] 


networks_no_pos <- systematic_formatted$analyzed_networks[[2]]
networks_pos <- systematic_formatted$analyzed_networks[[9002]]
all_networks <- list(networks_no_pos[,,2],
                     networks_pos[,,2])

netwobjects <- lapply(all_networks, function(x) network(x, directed = T))
layout_fixed <- readRDS( "../Results/Exploration/layout_network_graphs.RData")

#### Visualizations with only negative error ###  
t2_error <- netwobjects[[1]]
set.vertex.attribute(t2_error, "Sex", as.character(sex))

# Assign fixed layout coordinates to the second network
set.vertex.attribute(t2_error, "x", layout_fixed[, 1])
set.vertex.attribute(t2_error, "y", layout_fixed[, 2])

# Timepoint 2 visualization
ggnet2(t2_error, node.color = "Sex",
       arrow.size = 3, arrow.gap = 0.015, 
       node.size = 4, legend.position = "right",
       mode = c("x", "y")) +
  guides(size = F) +
  scale_color_manual(
    values = c("1" = "pink", "0" = "lightblue"),
    labels = c("1" = "Girls", "0" = "Boys"),
    name = "Sex") +
  ggtitle("End network - Neg = 0.36, pos = 0") 
  theme(plot.title = element_text(face = "bold"))
  
#### Visualizations with both errors ###  
t2_pos <- netwobjects[[2]]
set.vertex.attribute(t2_pos, "Sex", as.character(sex))

# Assign fixed layout coordinates to the second network
set.vertex.attribute(t2_pos, "x", layout_fixed[, 1])
set.vertex.attribute(t2_pos, "y", layout_fixed[, 2])

# Timepoint 2 visualization
ggnet2(t2_pos, node.color = "Sex",
       arrow.size = 3, arrow.gap = 0.015, 
       node.size = 4, legend.position = "right",
       mode = c("x", "y")) +
  guides(size = F) +
  scale_color_manual(
    values = c("1" = "pink", "0" = "lightblue"),
    labels = c("1" = "Girls", "0" = "Boys"),
    name = "Sex") +
  ggtitle("End network - Neg = 0.36, pos = 0.001") 
  theme(plot.title = element_text(face = "bold"))

```
## Explore triad census and Homophily
```{r}
# Assess the average change of the triad census over time
triad_keys <- c("003", "012", "102", "021D", "021U", "021C", "111D", "111U", "030T", "030C", 
                "201", "120D", "120U", "120C", "210", "300")

triad_census_names <- c("error_pos", "error_neg", "netw", triad_keys)
triad_census_df  <- data.frame(matrix(ncol = length(triad_census_names), nrow = 36))

homophily_names <- c("error_pos", "error_neg", "netw", "Homophily")
homophily_df <- data.frame(matrix(ncol = length(homophily_names), nrow = 36))

# Calculate average triad statistics, and homophily
for (i in 1:nrow(error_combo)){
  neg <- error_combo[i, 1]
  pos <- error_combo[i, 2]
  
  error_indices <- intersect(which(systematic_formatted$error_neg == neg), 
                             which(systematic_formatted$error_pos == pos))
  
  data <- systematic_formatted$analyzed_networks[error_indices]
  all_triads_1 <- sapply(lapply(data, function(arr) graph_from_adjacency_matrix(arr[,,1])), triad_census)
  all_triads_2 <- sapply(lapply(data, function(arr) graph_from_adjacency_matrix(arr[,,2])), triad_census)
  homophily_1 <- sapply(lapply(data, function(arr) arr[,,1]), function(x) PropSameSex(x, sex))
  homophily_2 <- sapply(lapply(data, function(arr) arr[,,1]), function(x) PropSameSex(x, sex))
  
  triad_census_df[i,] <- c(pos, neg, "netw1", rowMeans(all_triads_1))
  triad_census_df[i+18,] <- c(pos, neg, "netw2", rowMeans(all_triads_2))
  homophily_df[i,] <- c(pos, neg, "netw1", mean(homophily_1))
  homophily_df[i+18,] <- c(pos, neg, "netw2", mean(homophily_2))
}

colnames(triad_census_df) <- triad_census_names
colnames(homophily_df) <- homophily_names
write_xlsx(list(Triad_Census = triad_census_df, 
           Homophily = homophily_df),
           "../Results/Systematic/ExtraResults.xlsx")

# Plot results triad census
triad_columns <- colnames(triad_census_df)[!(colnames(triad_census_df) %in% 
                                               c("error_pos", "error_neg", "netw"))]
df_triads_long <- triad_census_df %>%
  mutate(across(-netw, as.numeric)) %>%
  pivot_longer(cols = all_of(triad_columns), names_to = "triad_type", values_to = "count") %>%
  group_by(triad_type) %>%
  mutate(
    # Standardize the count per triad type
    count_std = (count - mean(count, na.rm = TRUE)) / sd(count, na.rm = TRUE)
  )

label_axes <- labeller(netw = c(netw1 = "Start Network", netw2 = "End Network"), 
                                 error_pos = c("0" = "Positive Error: .000", "0.001" = 
                                                 "Positive Error: .001"))

triad_plot <- ggplot(df_triads_long, aes(x = error_neg, y = count_std, color = triad_type)) +
  geom_line() +
  scale_color_manual(values = c(
    brewer.pal(9, "Set1"),
    brewer.pal(7, "Set2"))) +
  facet_grid(netw ~ error_pos, scales = "free_y", labeller = label_axes) +
  labs(
    title = "",
    x = "Amount of (Baseline) Negative Error",
    y = "Standardized Triad Count",
    color = "Triad Type"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")  +
  theme(axis.title.y = element_text(margin = margin(r = 15), size = 15),
      axis.title.x = element_text(margin = margin(t = 15), size = 15),
      strip.text = element_text(size = 14))


ggsave("../Results/Systematic/triad_census_plot.jpg", plot = triad_plot, width = 16, height = 10, dpi = 300)
```